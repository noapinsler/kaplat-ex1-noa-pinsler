{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noapinsler/kaplat-ex1-noa-pinsler/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3TB_rjeVMOf",
        "outputId": "79d3e110-4d90-4873-f7d7-07ddc557652e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.22)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.49.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.22)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.49.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "/content\n",
            "Rock-Paper-Scissor-3  Rock-Paper-Scissors-SXSW-11  Rock-Paper-Scissors-SXSW-12\truns  sample_data\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.datasets import CocoDetection\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"LD4KL13wmatRF2nP1rca\")\n",
        "project = rf.workspace(\"rod-ian-baguio-m37qf\").project(\"rock-paper-scissor-7pbsy\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"coco\")\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oDsobjHVNBE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pycocotools.coco import COCO\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def get_transform(train=True):\n",
        "    if train:\n",
        "        return A.Compose([\n",
        "            A.Flip(0.5),\n",
        "            A.RandomRotate90(),\n",
        "            A.OneOf([\n",
        "                A.MotionBlur(p=0.2),\n",
        "                A.MedianBlur(blur_limit=3, p=0.1),\n",
        "                A.Blur(blur_limit=3, p=0.1),\n",
        "            ], p=0.2),\n",
        "            A.CLAHE(clip_limit=2),\n",
        "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()],\n",
        "            bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()],\n",
        "            bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),\n",
        "        )\n",
        "\n",
        "\n",
        "class CocoDetectionAlbumentations(Dataset):\n",
        "    def __init__(self, root, annotation, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.coco = COCO(annotation)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        annotations = coco.loadAnns(ann_ids)\n",
        "        img_info = coco.loadImgs(img_id)[0]\n",
        "        path = os.path.join(self.root, img_info['file_name'])\n",
        "\n",
        "        image = cv2.imread(path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image not found at path: {path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        height, width, _ = image.shape  # Get image dimensions\n",
        "\n",
        "        boxes = [ann['bbox'] for ann in annotations]\n",
        "        # Convert COCO format (top-left x, top-left y, width, height) to Pascal VOC format (x_min, y_min, x_max, y_max)\n",
        "        # and normalize the coordinates to be fractions of the image width and height\n",
        "        boxes = [[(box[0] / width), (box[1] / height),\n",
        "                  ((box[0] + box[2]) / width), ((box[1] + box[3]) / height)] for box in boxes]\n",
        "\n",
        "        labels = [ann['category_id'] for ann in annotations]\n",
        "        area = [ann['area'] for ann in annotations]\n",
        "        iscrowd = [ann['iscrowd'] for ann in annotations]\n",
        "\n",
        "        # Prepare target dictionary\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
        "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
        "        target[\"image_id\"] = torch.tensor([img_id])\n",
        "        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32) if area else torch.zeros((0,), dtype=torch.float32)\n",
        "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64) if iscrowd else torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': target['boxes'].tolist(),  # Make sure bboxes are in list format\n",
        "                'category_ids': target['labels'].tolist()  # Use 'category_ids' to match the label_fields in transformation\n",
        "            }\n",
        "            transformed = self.transforms(**sample)\n",
        "            image = transformed['image']\n",
        "\n",
        "            # Update the target dictionary with the transformed data\n",
        "            if 'bboxes' in transformed and transformed['bboxes']:\n",
        "                target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "            else:\n",
        "                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            # Note: No need to update labels here unless they are modified by transformations, which is uncommon\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfwB3C-xWR7p",
        "outputId": "9a0b58bf-ca0d-4dd9-d404-2824665e2467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "def collate_fn(batch):\n",
        "    # Filter out items where either the image or target is None\n",
        "    batch = [item for item in batch if item[0] is not None and item[1] is not None]\n",
        "\n",
        "    images, targets = zip(*batch)\n",
        "    images = list(images)\n",
        "    targets = list(targets)\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "\n",
        "dataset_root = '/content/Rock-Paper-Scissor-3'\n",
        "\n",
        "train_root = os.path.join(dataset_root, 'train')\n",
        "train_annFile = os.path.join(train_root, '_annotations.coco.json')\n",
        "\n",
        "valid_root = os.path.join(dataset_root, 'valid')\n",
        "valid_annFile = os.path.join(valid_root, '_annotations.coco.json')\n",
        "\n",
        "# Adjust if you have a test split; otherwise, you can ignore or comment out the test dataset setup\n",
        "test_root = os.path.join(dataset_root, 'test')\n",
        "test_annFile = os.path.join(test_root, '_annotations.coco.json')\n",
        "\n",
        "train_dataset = CocoDetectionAlbumentations(root=train_root,\n",
        "                                             annotation=train_annFile, transforms=get_transform(train=True))\n",
        "\n",
        "valid_dataset = CocoDetectionAlbumentations(root=valid_root,\n",
        "                                            annotation=valid_annFile,\n",
        "                                            transforms=get_transform(train=False))\n",
        "\n",
        "# If you have a test split\n",
        "test_dataset = CocoDetectionAlbumentations(root=test_root,\n",
        "                                           annotation=test_annFile,\n",
        "                                           transforms=get_transform(train=False))\n",
        "# DataLoader setup is correct as provided\n",
        "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=5, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=1, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22JbaAf9dSJe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXbrDVJMaFwu"
      },
      "outputs": [],
      "source": [
        "def get_backbone(num_classes):\n",
        "    # Load a pre-trained ResNet50 model\n",
        "    backbone = models.resnet50(pretrained=True)\n",
        "\n",
        "    # Remove the fully connected layer; Faster R-CNN will add its own\n",
        "    backbone_features = nn.Sequential(*list(backbone.children())[:-2])\n",
        "\n",
        "    # Update output channels attribute\n",
        "    backbone_features.out_channels = 2048\n",
        "\n",
        "    return backbone_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knjiXgR4dwF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef337c6-c85a-4bba-8323-7dd619c58638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        }
      ],
      "source": [
        "def get_object_detection_model(num_classes):\n",
        "    # Load a pre-trained model\n",
        "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # Replace the classifier with a new one for num_classes (num_classes + background)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming num_classes includes the background\n",
        "model = get_object_detection_model(num_classes=4)  # Update based on your datase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZNpyClSuAl8"
      },
      "outputs": [],
      "source": [
        "  import torch\n",
        "  from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "  def train_object_detection_model(num_epochs, train_loader, val_loader, model, criterion, optimizer, scheduler=None):\n",
        "      # Initialize TensorBoard writer\n",
        "      writer = SummaryWriter()\n",
        "\n",
        "      # Training loop\n",
        "      for epoch in range(num_epochs):\n",
        "          print(\"\\n----Training!----\")\n",
        "          model.train()\n",
        "          running_loss = 0.0\n",
        "          for images, targets in train_loader:\n",
        "\n",
        "              print(\"Batch size:\", len(images))\n",
        "              for i, target in enumerate(targets):\n",
        "                print(f\"Image {i} - Boxes: {target['boxes'].shape}, Labels: {target['labels'].shape}\")\n",
        "\n",
        "              print(\"Batch Targets:\", targets)\n",
        "              print(\"Target types:\", [type(t) for t in targets])\n",
        "              images = [img.to(device) for img in images]\n",
        "              targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              loss_dict = model(images, targets)\n",
        "              losses = sum(loss for loss in loss_dict.values())  # This is your total loss for the batch\n",
        "              losses.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              running_loss += losses.item() * len(images)\n",
        "\n",
        "          # Compute average training loss for the epoch\n",
        "          train_loss = running_loss / len(train_loader.dataset)\n",
        "          print(\"\\n----Done!----\")\n",
        "\n",
        "\n",
        "          # Validation loop\n",
        "          model.eval()\n",
        "          val_loss = 0.0\n",
        "          with torch.no_grad():\n",
        "              for images, targets in val_loader:\n",
        "                  print(\"Batch Targets:\", targets)\n",
        "                  print(\"Target types:\", [type(t) for t in targets])\n",
        "                  images = [img.to(device) for img in images]\n",
        "                  targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "\n",
        "                  # Forward pass\n",
        "                  outputs = model(images)\n",
        "                  print(\"outpus\",outputs)\n",
        "                  print(\"targets\",targets)\n",
        "\n",
        "                  # Calculate loss using custom loss function\n",
        "                  loss = custom_object_detection_loss(outputs, targets)\n",
        "\n",
        "                  # Update validation loss\n",
        "                  val_loss += loss.item() * len(images)\n",
        "\n",
        "          # Compute average validation loss for the epoch\n",
        "          val_loss /= len(val_loader.dataset)\n",
        "          # Log metrics to TensorBoard\n",
        "          writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "          writer.add_scalar('Loss/validation', val_loss, epoch)\n",
        "\n",
        "          # Print epoch statistics\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
        "\n",
        "          # Adjust learning rate if scheduler is provided\n",
        "          if scheduler is not None:\n",
        "              scheduler.step()\n",
        "\n",
        "      # Close TensorBoard writer\n",
        "      writer.close()\n",
        "\n",
        "      print('Training complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_object_detection_loss(outputs, targets):\n",
        "    if not isinstance(outputs, list):\n",
        "        raise ValueError(\"outputs must be a list\")\n",
        "\n",
        "    # Extract bounding box regression and classification outputs from the model\n",
        "    pred_boxes = outputs[0]['boxes']  # Assuming the first element contains bounding box predictions\n",
        "    pred_labels = outputs[0]['labels']  # Assuming the\n",
        "\n",
        "    # Extract ground truth bounding boxes and labels from the targets\n",
        "    true_boxes = targets[0]['boxes']\n",
        "    true_labels = targets[0]['labels']\n",
        "\n",
        "    # Compute smooth L1 loss for bounding box regression\n",
        "    box_loss = torch.nn.functional.smooth_l1_loss(pred_boxes, true_boxes)\n",
        "\n",
        "    # Compute cross-entropy loss for classification\n",
        "    cls_loss = torch.nn.functional.cross_entropy(pred_labels, true_labels)\n",
        "\n",
        "    # Combine the losses (you can adjust the weights as needed)\n",
        "    total_loss = box_loss + cls_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "oTx7HaSHNlRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ola8MT9gvYP7"
      },
      "outputs": [],
      "source": [
        "# Define your model, criterion, optimizer, and scheduler (if any)\n",
        "model = get_object_detection_model(num_classes=4)  # Example model\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Example loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Example scheduler\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Call the training function\n",
        "train_object_detection_model(num_epochs=10, train_loader=train_loader, val_loader=val_loader, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBIv8IaN_gPB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "print( torch.cuda.is_available())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxSNguy/G1emY76fAX/G3t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}